{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOMvNVAythAICdFEgktrlyO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SUREN37/Prompt-Engineering/blob/main/Support_Ticket_Categorization_Mistral_7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the required Packages and Libraries:"
      ],
      "metadata": {
        "id": "LjyWCrGd6Qes"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_MMjLyQauJn"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mla6Km0t6NJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "\n",
        "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "model_basename = \"mistral-7b-instruct-v0.2.Q5_K_M.gguf\"\n",
        "model_path = hf_hub_download(\n",
        "    repo_id = model_name,\n",
        "    filename = model_basename\n",
        ")\n",
        "lcpp_llm = Llama(\n",
        "    model_path = model_path,\n",
        "    n_threads = 2,\n",
        "    n_batch = 512,\n",
        "    n_gpu_layers = 43,\n",
        "    n_ctx = 4096\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23jT1OQMbxDQ",
        "outputId": "70ceb2e6-3390-4b8f-d82f-83ccf118583e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q5_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: mistral-instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Define a system message as a string and assign it to the variable system_message.\n",
        "import json"
      ],
      "metadata": {
        "id": "b-YxjkZgcItR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VL-d1EHJXLyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"\n",
        "Classify support tickets in the input into the appropriate category.\n",
        "The ticket will be delimited by triple backticks, that is, ```\n",
        "Do not explain your answer\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "0rYW6aCDSfdd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = '''\n",
        "[\n",
        "  {\n",
        "    \"user_input\": \"My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\",\n",
        "    \"assistant_output\": {\n",
        "      \"category\": \"Technical issues\",\n",
        "      \"tags\": [\"Internet connection\", \"Slow internet\", \"Disconnections\"],\n",
        "      \"priority\": \"High\",\n",
        "      \"eta\": \"Within 4 hours\",\n",
        "      \"response\": \"Thank you for reaching out to us regarding your internet connectivity issues. We apologize for any inconvenience caused by the slow speeds and frequent disconnections. Our team is working diligently to address the matter as soon as possible.\"\n",
        "    }\n",
        "  }\n",
        "]\n",
        "'''"
      ],
      "metadata": {
        "id": "CzZdS2y2YrL6"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_turn_template = \"\"\"<s>[INST]\\n <<SYS>> \\n {system_message} \\n <</SYS>>```{user_message}``` /n [/INST] \\n{assistant_message}\\n</s> \"\"\"\n",
        "examples_template = \"\"\"<s>[INST]\\n ```{user_message}``` \\n [/INST] \\n {assistant_message}\\n</s>\"\"\"\n",
        "prediction_template = \"\"\"<s>[INST]\\n ```{user_message}```[/INST]\"\"\"\n"
      ],
      "metadata": {
        "id": "0qwLiH-KZ6Ja"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_few_shot_examples(system_message, examples):\n",
        "\n",
        "    \"\"\"\n",
        "    This function is to create a few shot prompt for Support ticket categorization\n",
        "    Args:\n",
        "        system_message (str): system message with instructions for sentiment analysis\n",
        "        examples (str): JSON string with list of examples\n",
        "    Output:\n",
        "        few_shot_prompt (str) : a string containing system message, user input and assistant output examples.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "      few_shot_prompt = ''\n",
        "\n",
        "      for idx,example in enumerate(json.loads(examples)):\n",
        "          example_review = example['user_input']\n",
        "          example_output = example['assistant_output']\n",
        "\n",
        "          if idx == 0:\n",
        "              few_shot_prompt += first_turn_template.format(\n",
        "                  system_message=system_message,\n",
        "                  user_message=example_review,\n",
        "                  assistant_message=example_output\n",
        "              )\n",
        "          else:\n",
        "              few_shot_prompt += examples_template.format(\n",
        "                  user_message=example_review,\n",
        "                  assistant_message= example_output\n",
        "              )\n",
        "\n",
        "      return few_shot_prompt\n",
        "\n",
        "    except Exception as err:\n",
        "\n",
        "      return {'status':\"Failed at creating few shot prompt\" , 'Error': err}"
      ],
      "metadata": {
        "id": "zeFR4i0kZ6Rt"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_examples = create_few_shot_examples(system_message,examples)"
      ],
      "metadata": {
        "id": "tG61vVZ7XCCD"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(few_shot_examples,new_review):\n",
        "  \"\"\"\n",
        "  This function is to generate a prompt for support ticket categorization\n",
        "\n",
        "  Args:\n",
        "  few_shot_examples\n",
        "  new_review : user Input\n",
        "\n",
        "  Output: prompt for support ticket categorization\n",
        "  \"\"\"\n",
        "  try:\n",
        "\n",
        "    prompt = few_shot_examples+prediction_template.format(user_message=new_review)\n",
        "\n",
        "    return prompt\n",
        "\n",
        "  except Exception as err:\n",
        "\n",
        "    return {'status':'Failed in generating prompt','Error':err}"
      ],
      "metadata": {
        "id": "z4h3ZFqfXCES"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mistral_response(user_input):\n",
        "\n",
        "  \"\"\" This function Generates the Response from the Mistral model for the given prompt \"\"\"\n",
        "\n",
        "  try:\n",
        "\n",
        "    prompt=generate_prompt(few_shot_examples,user_input)\n",
        "\n",
        "    response = lcpp_llm(prompt=prompt,max_tokens=1024,\n",
        "               temperature=0,top_p=0.95,repeat_penalty=1.2,\n",
        "               top_k=50,stop=['INST'],echo=False)\n",
        "\n",
        "    response_text = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    return response_text\n",
        "\n",
        "  except Exception as err:\n",
        "\n",
        "    return {'status':'Failed in generating response','Error':err}"
      ],
      "metadata": {
        "id": "YzmkbTmXXCGn"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['mistral_response'] = data['support_ticket_text'].apply(lambda x : generate_mistral_response(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyWCDmRbXCKj",
        "outputId": "ac8fc96c-ed78-435a-d270-40b87884231e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1054: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =     129.37 ms /   188 runs   (    0.69 ms per token,  1453.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =  121088.26 ms /   262 tokens (  462.17 ms per token,     2.16 tokens per second)\n",
            "llama_print_timings:        eval time =  113941.11 ms /   187 runs   (  609.31 ms per token,     1.64 tokens per second)\n",
            "llama_print_timings:       total time =  235331.55 ms /   449 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =      87.61 ms /   125 runs   (    0.70 ms per token,  1426.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =   24319.82 ms /    52 tokens (  467.69 ms per token,     2.14 tokens per second)\n",
            "llama_print_timings:        eval time =   75391.06 ms /   124 runs   (  607.99 ms per token,     1.64 tokens per second)\n",
            "llama_print_timings:       total time =   99893.04 ms /   176 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =     467.89 ms /   679 runs   (    0.69 ms per token,  1451.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =   22100.95 ms /    47 tokens (  470.23 ms per token,     2.13 tokens per second)\n",
            "llama_print_timings:        eval time =  419792.75 ms /   678 runs   (  619.16 ms per token,     1.62 tokens per second)\n",
            "llama_print_timings:       total time =  443358.65 ms /   725 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =     115.53 ms /   183 runs   (    0.63 ms per token,  1584.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =   27057.87 ms /    59 tokens (  458.61 ms per token,     2.18 tokens per second)\n",
            "llama_print_timings:        eval time =  110475.76 ms /   182 runs   (  607.01 ms per token,     1.65 tokens per second)\n",
            "llama_print_timings:       total time =  137812.23 ms /   241 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =     228.32 ms /   337 runs   (    0.68 ms per token,  1476.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14913.22 ms /    32 tokens (  466.04 ms per token,     2.15 tokens per second)\n",
            "llama_print_timings:        eval time =  205990.41 ms /   336 runs   (  613.07 ms per token,     1.63 tokens per second)\n",
            "llama_print_timings:       total time =  221481.53 ms /   368 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =      86.07 ms /   132 runs   (    0.65 ms per token,  1533.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13954.92 ms /    30 tokens (  465.16 ms per token,     2.15 tokens per second)\n",
            "llama_print_timings:        eval time =   78560.61 ms /   131 runs   (  599.70 ms per token,     1.67 tokens per second)\n",
            "llama_print_timings:       total time =   92720.51 ms /   161 tokens\n",
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7gAmtyxaj3b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDozV47Uj3ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Ht8dXHYj3ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "41egzBugj3mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnPswPocXCQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a zero shot prompt template that incorporates the system message and user input."
      ],
      "metadata": {
        "id": "Xja1_4UbeJHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_template = \"\"\"<s>[INST]\\n <<SYS>> \\n {system_message} \\n <</SYS>>```{user_message}``` /n [/INST] \"\"\""
      ],
      "metadata": {
        "id": "jiLcpiQHeJQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define generate_prompt function that takes both the system_message and user_input as arguments and formats them into a prompt template (1)."
      ],
      "metadata": {
        "id": "A5toDxd6eJTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(system_message,user_input):\n",
        "\n",
        "  \"\"\" This function Generate the prompt by getting the system message and user input as Arguments \"\"\"\n",
        "  try:\n",
        "\n",
        "    prompt=zero_shot_template.format(system_message=system_message,user_message=user_input)\n",
        "    return prompt\n",
        "\n",
        "  except Exception as err:\n",
        "\n",
        "    return \"Failed in Prompt Generation\"\n"
      ],
      "metadata": {
        "id": "MWUOO_3P9cJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "biXXD49kJDPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python function called generate_mistral_response that takes a single parameter, support_ticket_text, which represents the user's support ticket text."
      ],
      "metadata": {
        "id": "8FrOt1Ry9cSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mistral_response(user_input):\n",
        "\n",
        "  \"\"\" This function Generates the Response from the Mistral model for the given prompt \"\"\"\n",
        "\n",
        "  try:\n",
        "\n",
        "    prompt=generate_prompt(few_shot_examples,user_input)\n",
        "\n",
        "    response = lcpp_llm(prompt=prompt,max_tokens=1024,\n",
        "               temperature=0,top_p=0.95,repeat_penalty=1.2,\n",
        "               top_k=50,stop=['INST'],echo=False)\n",
        "\n",
        "    response_text = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    return response_text\n",
        "\n",
        "  except Exception as err:\n",
        "\n",
        "    return \"Failed in generate Mistral response\""
      ],
      "metadata": {
        "id": "Ib3wnv4y9cVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Dataset:\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "NKKZ0Px49cYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Support_ticket_text_data_mid_term.csv')"
      ],
      "metadata": {
        "id": "2qR-lzUH9cav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GnRr0EDE9cc8",
        "outputId": "4695305e-0f21-43fc-8fd6-14682266061d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  support_tick_id                                support_ticket_text\n",
              "0      ST2023-006  My internet connection has significantly slowe...\n",
              "1      ST2023-007  Urgent help required! My laptop refuses to sta...\n",
              "2      ST2023-008  I've accidentally deleted essential work docum...\n",
              "3      ST2023-009  Despite being in close proximity to my Wi-Fi r...\n",
              "4      ST2023-010  My smartphone battery is draining rapidly, eve..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-432e39b8-39d5-42d3-9790-3588a67374ad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>support_tick_id</th>\n",
              "      <th>support_ticket_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ST2023-006</td>\n",
              "      <td>My internet connection has significantly slowe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ST2023-007</td>\n",
              "      <td>Urgent help required! My laptop refuses to sta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ST2023-008</td>\n",
              "      <td>I've accidentally deleted essential work docum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ST2023-009</td>\n",
              "      <td>Despite being in close proximity to my Wi-Fi r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ST2023-010</td>\n",
              "      <td>My smartphone battery is draining rapidly, eve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-432e39b8-39d5-42d3-9790-3588a67374ad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-432e39b8-39d5-42d3-9790-3588a67374ad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-432e39b8-39d5-42d3-9790-3588a67374ad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2f47420d-49bb-424b-b34f-fb3e83fe8a72\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2f47420d-49bb-424b-b34f-fb3e83fe8a72')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2f47420d-49bb-424b-b34f-fb3e83fe8a72 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 21,\n  \"fields\": [\n    {\n      \"column\": \"support_tick_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          \"ST2023-006\",\n          \"ST2023-023\",\n          \"ST2023-021\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"support_ticket_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          \"My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\",\n          \"I accidentally formatted my USB drive with crucial files. Help needed for data recovery.\",\n          \"My internet connection is frequently dropping, affecting my work. Urgent help needed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4 - Get the response by giving the arguments to mistral model"
      ],
      "metadata": {
        "id": "Q7v6FIlE9cfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['mistral_response'] = data['support_ticket_text'].apply(lambda x : generate_mistral_response(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1VZ_jTsMMH9",
        "outputId": "232c9c62-ccf5-44f3-e12d-0f0be8c5d0fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1054: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       2.75 ms /     4 runs   (    0.69 ms per token,  1456.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =   59161.91 ms /   123 tokens (  480.99 ms per token,     2.08 tokens per second)\n",
            "llama_print_timings:        eval time =    1669.73 ms /     3 runs   (  556.58 ms per token,     1.80 tokens per second)\n",
            "llama_print_timings:       total time =   60837.61 ms /   126 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =     136.35 ms /   207 runs   (    0.66 ms per token,  1518.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25447.79 ms /    55 tokens (  462.69 ms per token,     2.16 tokens per second)\n",
            "llama_print_timings:        eval time =  124641.57 ms /   206 runs   (  605.06 ms per token,     1.65 tokens per second)\n",
            "llama_print_timings:       total time =  150420.89 ms /   261 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       2.61 ms /     4 runs   (    0.65 ms per token,  1534.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =   23294.55 ms /    50 tokens (  465.89 ms per token,     2.15 tokens per second)\n",
            "llama_print_timings:        eval time =    1579.03 ms /     3 runs   (  526.34 ms per token,     1.90 tokens per second)\n",
            "llama_print_timings:       total time =   24879.25 ms /    53 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       3.77 ms /     6 runs   (    0.63 ms per token,  1590.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =   28299.65 ms /    62 tokens (  456.45 ms per token,     2.19 tokens per second)\n",
            "llama_print_timings:        eval time =    2628.86 ms /     5 runs   (  525.77 ms per token,     1.90 tokens per second)\n",
            "llama_print_timings:       total time =   30935.70 ms /    67 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       8.31 ms /    13 runs   (    0.64 ms per token,  1563.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =   16471.18 ms /    35 tokens (  470.61 ms per token,     2.12 tokens per second)\n",
            "llama_print_timings:        eval time =    6931.20 ms /    12 runs   (  577.60 ms per token,     1.73 tokens per second)\n",
            "llama_print_timings:       total time =   23418.63 ms /    47 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =      11.16 ms /     7 runs   (    1.59 ms per token,   627.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15069.69 ms /    33 tokens (  456.66 ms per token,     2.19 tokens per second)\n",
            "llama_print_timings:        eval time =    4549.58 ms /     6 runs   (  758.26 ms per token,     1.32 tokens per second)\n",
            "llama_print_timings:       total time =   19634.39 ms /    39 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       2.54 ms /     4 runs   (    0.64 ms per token,  1574.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =   16352.64 ms /    36 tokens (  454.24 ms per token,     2.20 tokens per second)\n",
            "llama_print_timings:        eval time =    1569.04 ms /     3 runs   (  523.01 ms per token,     1.91 tokens per second)\n",
            "llama_print_timings:       total time =   17927.06 ms /    39 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =      68.29 ms /    89 runs   (    0.77 ms per token,  1303.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17244.67 ms /    38 tokens (  453.81 ms per token,     2.20 tokens per second)\n",
            "llama_print_timings:        eval time =   53855.27 ms /    88 runs   (  611.99 ms per token,     1.63 tokens per second)\n",
            "llama_print_timings:       total time =   71232.18 ms /   126 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       2.64 ms /     4 runs   (    0.66 ms per token,  1515.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15217.50 ms /    33 tokens (  461.14 ms per token,     2.17 tokens per second)\n",
            "llama_print_timings:        eval time =    1604.40 ms /     3 runs   (  534.80 ms per token,     1.87 tokens per second)\n",
            "llama_print_timings:       total time =   16826.59 ms /    36 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       5.11 ms /     8 runs   (    0.64 ms per token,  1565.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =   19136.89 ms /    40 tokens (  478.42 ms per token,     2.09 tokens per second)\n",
            "llama_print_timings:        eval time =    3912.92 ms /     7 runs   (  558.99 ms per token,     1.79 tokens per second)\n",
            "llama_print_timings:       total time =   23059.28 ms /    47 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       2.55 ms /     4 runs   (    0.64 ms per token,  1570.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15984.92 ms /    35 tokens (  456.71 ms per token,     2.19 tokens per second)\n",
            "llama_print_timings:        eval time =    1629.65 ms /     3 runs   (  543.22 ms per token,     1.84 tokens per second)\n",
            "llama_print_timings:       total time =   17619.54 ms /    38 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       3.73 ms /     6 runs   (    0.62 ms per token,  1609.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =   18748.41 ms /    39 tokens (  480.73 ms per token,     2.08 tokens per second)\n",
            "llama_print_timings:        eval time =    2621.91 ms /     5 runs   (  524.38 ms per token,     1.91 tokens per second)\n",
            "llama_print_timings:       total time =   21377.36 ms /    44 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       7.07 ms /    11 runs   (    0.64 ms per token,  1555.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17220.09 ms /    38 tokens (  453.16 ms per token,     2.21 tokens per second)\n",
            "llama_print_timings:        eval time =    6646.11 ms /    10 runs   (  664.61 ms per token,     1.50 tokens per second)\n",
            "llama_print_timings:       total time =   23880.62 ms /    48 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       2.50 ms /     4 runs   (    0.63 ms per token,  1598.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13183.93 ms /    29 tokens (  454.62 ms per token,     2.20 tokens per second)\n",
            "llama_print_timings:        eval time =    1603.33 ms /     3 runs   (  534.44 ms per token,     1.87 tokens per second)\n",
            "llama_print_timings:       total time =   14792.01 ms /    32 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       4.49 ms /     7 runs   (    0.64 ms per token,  1558.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =   16430.93 ms /    36 tokens (  456.41 ms per token,     2.19 tokens per second)\n",
            "llama_print_timings:        eval time =    4569.43 ms /     6 runs   (  761.57 ms per token,     1.31 tokens per second)\n",
            "llama_print_timings:       total time =   21008.90 ms /    42 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       2.63 ms /     4 runs   (    0.66 ms per token,  1522.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11516.12 ms /    25 tokens (  460.64 ms per token,     2.17 tokens per second)\n",
            "llama_print_timings:        eval time =    2013.61 ms /     3 runs   (  671.20 ms per token,     1.49 tokens per second)\n",
            "llama_print_timings:       total time =   13535.03 ms /    28 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       2.44 ms /     4 runs   (    0.61 ms per token,  1638.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14590.87 ms /    32 tokens (  455.96 ms per token,     2.19 tokens per second)\n",
            "llama_print_timings:        eval time =    1593.01 ms /     3 runs   (  531.00 ms per token,     1.88 tokens per second)\n",
            "llama_print_timings:       total time =   16189.60 ms /    35 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       2.57 ms /     4 runs   (    0.64 ms per token,  1557.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12024.82 ms /    26 tokens (  462.49 ms per token,     2.16 tokens per second)\n",
            "llama_print_timings:        eval time =    1578.82 ms /     3 runs   (  526.27 ms per token,     1.90 tokens per second)\n",
            "llama_print_timings:       total time =   13607.82 ms /    29 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       2.52 ms /     4 runs   (    0.63 ms per token,  1588.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13002.22 ms /    28 tokens (  464.36 ms per token,     2.15 tokens per second)\n",
            "llama_print_timings:        eval time =    1605.56 ms /     3 runs   (  535.19 ms per token,     1.87 tokens per second)\n",
            "llama_print_timings:       total time =   14612.81 ms /    31 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =      25.88 ms /    40 runs   (    0.65 ms per token,  1545.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =   82267.60 ms /   178 tokens (  462.18 ms per token,     2.16 tokens per second)\n",
            "llama_print_timings:        eval time =   22714.99 ms /    39 runs   (  582.44 ms per token,     1.72 tokens per second)\n",
            "llama_print_timings:       total time =  105040.70 ms /   217 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   59162.77 ms\n",
            "llama_print_timings:      sample time =       4.42 ms /     7 runs   (    0.63 ms per token,  1585.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =  135666.21 ms /   292 tokens (  464.61 ms per token,     2.15 tokens per second)\n",
            "llama_print_timings:        eval time =    3306.59 ms /     6 runs   (  551.10 ms per token,     1.81 tokens per second)\n",
            "llama_print_timings:       total time =  138982.87 ms /   298 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LRk6tB82NDqS",
        "outputId": "e3adbf21-8be0-4d70-d2ac-1049fff8664d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  support_tick_id                                support_ticket_text  \\\n",
              "0      ST2023-006  My internet connection has significantly slowe...   \n",
              "1      ST2023-007  Urgent help required! My laptop refuses to sta...   \n",
              "2      ST2023-008  I've accidentally deleted essential work docum...   \n",
              "3      ST2023-009  Despite being in close proximity to my Wi-Fi r...   \n",
              "4      ST2023-010  My smartphone battery is draining rapidly, eve...   \n",
              "\n",
              "                                    mistral_response  \n",
              "0                                     Network Issues  \n",
              "1  Hardware Support\\n<</SYS>> ```I am having trou...  \n",
              "2                                      Data Recovery  \n",
              "3                                  1. Network Issues  \n",
              "4                     1. Hardware: Battery\\n<</SYS>>  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8539dbc-68cf-4572-bbec-78575227105e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>support_tick_id</th>\n",
              "      <th>support_ticket_text</th>\n",
              "      <th>mistral_response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ST2023-006</td>\n",
              "      <td>My internet connection has significantly slowe...</td>\n",
              "      <td>Network Issues</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ST2023-007</td>\n",
              "      <td>Urgent help required! My laptop refuses to sta...</td>\n",
              "      <td>Hardware Support\\n&lt;&lt;/SYS&gt;&gt; ```I am having trou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ST2023-008</td>\n",
              "      <td>I've accidentally deleted essential work docum...</td>\n",
              "      <td>Data Recovery</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ST2023-009</td>\n",
              "      <td>Despite being in close proximity to my Wi-Fi r...</td>\n",
              "      <td>1. Network Issues</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ST2023-010</td>\n",
              "      <td>My smartphone battery is draining rapidly, eve...</td>\n",
              "      <td>1. Hardware: Battery\\n&lt;&lt;/SYS&gt;&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8539dbc-68cf-4572-bbec-78575227105e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f8539dbc-68cf-4572-bbec-78575227105e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f8539dbc-68cf-4572-bbec-78575227105e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d912a0d2-bea0-4a38-b663-6123a4791d14\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d912a0d2-bea0-4a38-b663-6123a4791d14')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d912a0d2-bea0-4a38-b663-6123a4791d14 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 21,\n  \"fields\": [\n    {\n      \"column\": \"support_tick_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          \"ST2023-006\",\n          \"ST2023-023\",\n          \"ST2023-021\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"support_ticket_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          \"My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\",\n          \"I accidentally formatted my USB drive with crucial files. Help needed for data recovery.\",\n          \"My internet connection is frequently dropping, affecting my work. Urgent help needed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mistral_response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"Hardware (Display)\",\n          \" Hardware (Touchpad)\",\n          \"Network Issues\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mjEvyPA0RM7Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}